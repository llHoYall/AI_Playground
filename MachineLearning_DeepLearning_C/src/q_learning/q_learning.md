# Q Learning

강화 학습을 통하여 가장 보상이 높은 행동을 찾는다.

- R: Reward
- A: Action
- S: State

## Datasets

다음과 같은 상황을 가정한다.

```
            N1
        /        \
     N2            N3
   /    \        /     \
  N4     N5     N6      N7
 / \    / \     / \     / \
N8 N9 N10 N11 N12 N13 N14 N15
```

가장 상위의 노드부터 하위의 노드로 내려가는 데, 가장 보상이 높은 루트를 찾는다.

## Q 학습의 학습 절차

* 1. 모든 Q 값을 난수를 이용하여 초기화.
* 2. 학습이 충분히 진행될 때까지 다음을 반복.
  + 2.1. 동작의 초기 상태로 돌아감.
  + 2.2. 선택 가능한 행동 중에서 Q 값을 기반으로 다음 행동을 결정.
  + 2.3. 행동 후 식에따라 Q 값을 갱신.
    - 2.3.1. 혹시 보상을 얻었다면 보상에 비례하는 값을 Q 값에 더함.
    - 2.3.2. 다음 상태로 선택할 수 있는 행동에 대한 Q 값 중 최대값에 비례한 값을 Q 값에 더함.
  + 2.4. 조건 (목표 상태 또는 일정 시간 경과)에 다다르면 2.1.로 돌아감
  + 2.5. 2.2.로 돌아감

### Q값 갱신 식

`Q(s(t), a(t)) = Q(s(t), a(t)) + alpha * (r + gamma * Q(s(t+1), a(t+1)) - Q(s(t), a(t)))`

### epsilon-greedy method

- epsilon 만큼의 %로 무작위 행동 선택.
- 1 - epsilon 만큼의 %로 Q값 기반으로 행동 선택
